# GAN in Python on MNIST Dataset

This repository contains an implementation of a **Generative Adversarial Network (GAN)** for image generation using the MNIST dataset. The project demonstrates how a GAN can be used to generate realistic images of handwritten digits, leveraging Python and deep learning frameworks such as TensorFlow and Keras.

## Overview

A **Generative Adversarial Network (GAN)** is a class of machine learning frameworks designed to generate synthetic data similar to real data. In this case, we aim to generate images that resemble the handwritten digits found in the MNIST dataset, which consists of grayscale images of digits ranging from 0 to 9.

A GAN consists of two main components:

1. **The Generator:** This neural network takes a random noise vector as input and generates an image. The generator’s goal is to produce realistic images that are indistinguishable from real images in the dataset.

2. **The Discriminator:** This neural network evaluates images and predicts whether each one is real (from the dataset) or fake (generated by the generator). Its job is to correctly classify images as real or fake.

The two networks play an adversarial game:
- The **generator** tries to "fool" the discriminator by producing images that appear real.
- The **discriminator** tries to improve its ability to distinguish between real and generated (fake) images.

### How It Works

The generator and discriminator are trained simultaneously but with opposing objectives. As the generator improves at creating realistic images, the discriminator gets better at distinguishing real from fake. Over time, this process leads to a generator that can produce highly realistic images.

1. **Training Process:**
   - A random noise vector (also called latent vector) is passed to the generator, which outputs a generated image.
   - The discriminator evaluates both real images from the MNIST dataset and the generated images, predicting whether they are real or fake.
   - The discriminator is trained to maximize its accuracy in distinguishing between real and fake images, while the generator is trained to minimize the discriminator’s ability to tell the difference.
   - This results in two loss values: the **generator loss** (how well the generator is fooling the discriminator) and the **discriminator loss** (how well the discriminator is at detecting fakes).

2. **Loss Functions:**
   - The **discriminator** uses a binary cross-entropy loss function, treating the real images as class 1 (real) and the fake images as class 0 (fake).
   - The **generator** is trained to maximize the discriminator's probability of assigning class 1 (real) to the fake images, effectively minimizing its own binary cross-entropy loss.

3. **Model Architecture:**
   - **Generator:** The generator starts with a random noise input (latent space) and passes it through a series of dense and transposed convolutional layers. The output is a 28x28 grayscale image designed to resemble the MNIST digits.
   - **Discriminator:** The discriminator is a convolutional neural network (CNN) that takes an image as input (either real or fake) and outputs a probability score indicating whether the image is real or fake. It uses a sigmoid function at the output layer for binary classification.

4. **Adversarial Training:**  
   During training, the generator learns to create more realistic images, while the discriminator becomes increasingly skilled at identifying generated images. This adversarial process improves both networks, resulting in a generator capable of producing highly realistic handwritten digits.

## MNIST Dataset

The MNIST dataset consists of 70,000 images of handwritten digits (60,000 for training and 10,000 for testing). Each image is 28x28 pixels and represents a digit from 0 to 9. This dataset is a classic benchmark for image generation and classification tasks in machine learning.

## Results

After training, the generator is capable of producing images that closely resemble real MNIST digits. The generated images evolve throughout the training process, starting from random noise and gradually improving as the generator learns to model the distribution of the MNIST digits.

